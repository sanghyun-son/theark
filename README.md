# TheArk ğŸš€

[![CI](https://github.com/sanghyun-son/theark/actions/workflows/ci.yml/badge.svg)](https://github.com/sanghyun-son/theark/actions/workflows/ci.yml)
[![Python 3.11](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
[![Linting: ruff](https://img.shields.io/badge/linting-ruff-000000.svg)](https://github.com/astral-sh/ruff)

A comprehensive paper management system with ArXiv crawling, AI-powered summarization, and LLM request tracking. Built with modern Python practices and FastAPI for scalable backend services.

## âœ¨ Features

- **ğŸ” ArXiv Crawling**: Automated paper discovery and metadata extraction
- **ğŸ¤– AI Summarization**: OpenAI-powered abstract summarization with structured output
- **ğŸ“Š LLM Tracking**: Comprehensive monitoring of API usage and costs
- **ğŸŒ FastAPI Backend**: RESTful API server for frontend integration
- **ğŸ’¾ Database Management**: SQLite-based storage with LLM request tracking
- **ğŸ”’ Type Safety**: Full type hints with mypy strict checking
- **ğŸ§ª Comprehensive Testing**: Unit and integration tests with pytest
- **ğŸ“‹ Code Quality**: Automated formatting, linting, and type checking

## ğŸš€ Quick Start

### Prerequisites

- Python 3.11+
- [uv](https://github.com/astral-sh/uv) (recommended) or pip
- OpenAI API key (for summarization features)

### Installation

```bash
# Clone the repository
git clone <repository-url>
cd theark

# Install dependencies with uv (recommended)
uv sync

# Or with pip
pip install -e .

# Set up environment
export OPENAI_API_KEY="your-api-key-here"
```

### Running the API Server

```bash
# Start the FastAPI server (default settings)
./scripts/run/start.sh

# Start with custom port
./scripts/run/start.sh --port 3000

# Start in production mode
./scripts/run/start.sh --prod

# Get help with options
./scripts/run/start.sh --help

# Server will be available at http://localhost:8000
# API documentation at http://localhost:8000/docs
```

### API Endpoints

- `GET /` - Root endpoint with API information
- `GET /health` - Health check endpoint
- `GET /docs` - Interactive API documentation (Swagger UI)
- `GET /openapi.json` - OpenAPI schema
- `GET /favicon.ico` - Favicon file

### Running the Demo

```bash
# Run the comprehensive TheArk demo
uv run python examples/theark_demo.py
```

## ğŸ› ï¸ Development

### Code Quality

```bash
# Run all quality checks
./scripts/dev/check.sh
```

### Testing

```bash
# Run all tests
./scripts/dev/test.sh

# Run specific test categories
./scripts/dev/test.sh tests/api/           # API tests
./scripts/dev/test.sh tests/core/          # Core functionality tests

# Run with different options
./scripts/dev/test.sh --integration        # Integration tests only
./scripts/dev/test.sh --unit              # Unit tests only
./scripts/dev/test.sh --log-debug         # With debug logging
./scripts/dev/test.sh --verbose           # Verbose output
```

## ğŸ—ï¸ Architecture

### Core Components

- **ğŸ” ArXiv Crawler**: Handles paper discovery and metadata extraction
- **ğŸ¤– Summarization Service**: AI-powered abstract analysis
- **ğŸ“Š LLM Tracking**: Monitors API usage and calculates costs
- **ğŸŒ FastAPI Server**: RESTful API for frontend integration

### Database

- **ğŸ’¾ Main Database**: SQLite-based paper and summary storage
- **ğŸ“ˆ LLM Database**: Separate database for API request tracking

### Project Structure

```
theark/
â”œâ”€â”€ api/                    # FastAPI backend server
â”‚   â”œâ”€â”€ routers/           # API route modules
â”‚   â”œâ”€â”€ dependencies.py    # Dependency injection
â”‚   â””â”€â”€ templates/         # HTML templates
â”œâ”€â”€ core/                  # Core utilities and logging
â”œâ”€â”€ crawler/               # ArXiv crawling and processing
â”‚   â”œâ”€â”€ database/         # Database models and management
â”‚   â””â”€â”€ summarizer/       # AI summarization services
â”œâ”€â”€ static/               # Static files (favicon, etc.)
â”œâ”€â”€ tests/                # Test suite
â”œâ”€â”€ examples/             # Demo and example scripts
â””â”€â”€ db/                   # Database files
```

## ğŸ“‹ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

### Development Guidelines

- Follow PEP 8 style guidelines
- Add type hints to all functions
- Write tests for new functionality
- Run `./scripts/dev/check.sh` before committing
- Use `./scripts/dev/check.sh --fix` to auto-fix formatting issues
- Use meaningful commit messages

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸš€ **LLM Request ë¡œê¹… ì‹œìŠ¤í…œ**

### **ê°œìš”**
LLM Request ë¡œê¹… ì‹œìŠ¤í…œì€ ëª¨ë“  LLM API í˜¸ì¶œì„ ìë™ìœ¼ë¡œ ì¶”ì í•˜ê³  ë¶„ì„í•  ìˆ˜ ìˆê²Œ í•´ì£¼ëŠ” ë²”ìš©ì ì¸ ì†”ë£¨ì…˜ì…ë‹ˆë‹¤.

### **ì£¼ìš” ê¸°ëŠ¥**
- **ìë™ ë¡œê¹…**: Context Managerë¥¼ ì‚¬ìš©í•˜ì—¬ ìµœì†Œí•œì˜ ì½”ë“œë¡œ ìë™ ë¡œê¹…
- **ë¹„ìš© ì¶”ì **: OpenAI ê°€ê²© ì •ì±…ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ìë™ ë¹„ìš© ê³„ì‚°
- **ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§**: ì‘ë‹µ ì‹œê°„, í† í° ì‚¬ìš©ëŸ‰ ë“± ìƒì„¸í•œ ë©”íŠ¸ë¦­
- **ì—ëŸ¬ ì¶”ì **: ì‹¤íŒ¨í•œ ìš”ì²­ì— ëŒ€í•œ ìë™ ì—ëŸ¬ ë¡œê¹…
- **í™•ì¥ì„±**: ë‹¤ì–‘í•œ LLM ì‚¬ìš© ì‚¬ë¡€ì— ì‰½ê²Œ ì ìš© ê°€ëŠ¥

### **ì‚¬ìš© ì˜ˆì‹œ**

#### **1. ê¸°ë³¸ ì‚¬ìš©ë²• (SummarizationService í†µí•©)**
```python
from core.services import PaperSummarizationService

# ìë™ìœ¼ë¡œ LLM Requestê°€ ë¡œê¹…ë©ë‹ˆë‹¤
summary = await summarization_service.summarize_paper(
    paper=paper,
    db_session=db_session,  # ëŸ°íƒ€ì„ì— ì£¼ì…
    llm_client=llm_client,
    language="Korean"
)
```

#### **2. ì§ì ‘ ì‚¬ìš©ë²•**
```python
from core.services import track_llm_request

async with track_llm_request(
    db_session=db_session,
    model="gpt-4",
    custom_id="user-123-summary",
    request_type="summarization",
    metadata={"language": "Korean", "content_length": 1000}
) as tracker:
    # í•µì‹¬ ë¡œì§ë§Œ ì‹¤í–‰
    response = await llm_client.chat.completions.create(...)
    
    # ì‘ë‹µ ì„¤ì • (ìë™ ë¡œê¹…ì„ ìœ„í•´)
    tracker.set_response(response)
    
    return response
```

#### **3. ê³ ê¸‰ ì‚¬ìš©ë²•**
```python
from core.services import LLMRequestTracker

async with LLMRequestTracker(
    db_session=db_session,
    model="gpt-4",
    custom_id="custom-analysis",
    request_type="analysis",
    metadata={"domain": "AI", "complexity": "high"}
) as tracker:
    # ì»¤ìŠ¤í…€ ë©”íƒ€ë°ì´í„° ì¶”ê°€
    tracker.set_custom_metadata("user_preference", "detailed")
    
    # LLM ìš”ì²­ ì‹¤í–‰
    response = await execute_llm_request()
    
    # ì‘ë‹µ ì„¤ì •
    tracker.set_response(response)
    
    return response
```

### **ìë™ ë¡œê¹… ì •ë³´**
- **ê¸°ë³¸ ì •ë³´**: ëª¨ë¸, ì œê³µì, ì—”ë“œí¬ì¸íŠ¸, ìš”ì²­ íƒ€ì…
- **ì„±ëŠ¥ ë©”íŠ¸ë¦­**: ì‘ë‹µ ì‹œê°„, í† í° ì‚¬ìš©ëŸ‰
- **ë¹„ìš© ì •ë³´**: ì˜ˆìƒ ë¹„ìš© (USD)
- **ìƒíƒœ ì •ë³´**: ì„±ê³µ/ì‹¤íŒ¨/ì—ëŸ¬ ìƒíƒœ
- **ì»¤ìŠ¤í…€ ë°ì´í„°**: ì‚¬ìš©ì ì •ì˜ ë©”íƒ€ë°ì´í„°

### **ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ**
```sql
CREATE TABLE llm_request (
    id INTEGER PRIMARY KEY,
    timestamp TEXT NOT NULL,
    model TEXT NOT NULL,
    provider TEXT NOT NULL,
    endpoint TEXT NOT NULL,
    custom_id TEXT,
    request_type TEXT NOT NULL,
    status TEXT NOT NULL,
    prompt_tokens INTEGER,
    completion_tokens INTEGER,
    total_tokens INTEGER,
    estimated_cost_usd REAL,
    response_time_ms INTEGER,
    http_status_code INTEGER,
    error_message TEXT,
    metadata TEXT
);
```

### **ë¶„ì„ ë° ëª¨ë‹ˆí„°ë§**
```python
from core.database.repository import LLMRequestRepository

# ì¼ì¼ ë¹„ìš© ìš”ì•½ (Pydantic ëª¨ë¸ ë°˜í™˜)
cost_summary = llm_repo.get_cost_summary_by_date("2024-01-15")
print(f"ì´ ë¹„ìš©: ${cost_summary.total_cost_usd}")
print(f"ìš”ì²­ ìˆ˜: {cost_summary.request_count}")

# ëª¨ë¸ë³„ ì‚¬ìš© í†µê³„ (íƒ€ì… ì•ˆì „í•œ Pydantic ëª¨ë¸)
usage_stats = llm_repo.get_model_usage_stats("2024-01-01", "2024-01-31")
for model_name, stats in usage_stats.models.items():
    print(f"{model_name}: {stats.total_requests} requests, ${stats.total_cost_usd}")

# ê¸°ê°„ë³„ ì´ ë¹„ìš©
total_cost = llm_repo.get_total_cost_by_period("2024-01-01", "2024-01-31")
```

### **íƒ€ì… ì•ˆì „ì„± ë° ëª¨ë¸ ê¸°ë°˜ ì„¤ê³„**
- **Pydantic ëª¨ë¸**: `dict` ëŒ€ì‹  íƒ€ì… ì•ˆì „í•œ Pydantic ëª¨ë¸ ì‚¬ìš©
- **ìë™ ê²€ì¦**: ë°ì´í„°ë² ì´ìŠ¤ ì‘ë‹µì˜ ìë™ íƒ€ì… ê²€ì¦
- **IDE ì§€ì›**: ì™„ë²½í•œ ìë™ì™„ì„±ê³¼ íƒ€ì… íŒíŠ¸ ì§€ì›
- **í™•ì¥ì„±**: ìƒˆë¡œìš´ í†µê³„ í•„ë“œ ì¶”ê°€ ì‹œ ëª¨ë¸ë§Œ ìˆ˜ì •í•˜ë©´ ë¨

### **ì¥ì **
1. **ìµœì†Œí•œì˜ ì½”ë“œ**: `with` ë¬¸ ë‚´ë¶€ì—ëŠ” í•µì‹¬ ë¡œì§ë§Œ
2. **ìë™ ì—ëŸ¬ ì²˜ë¦¬**: ì˜ˆì™¸ ë°œìƒ ì‹œ ìë™ìœ¼ë¡œ ì—ëŸ¬ ë¡œê¹…
3. **ëŸ°íƒ€ì„ ì˜ì¡´ì„± ì£¼ì…**: DB ì„¸ì…˜ì„ `__init__`ì´ ì•„ë‹Œ ëŸ°íƒ€ì„ì— ì£¼ì…
4. **ê¸°ì¡´ ìƒìˆ˜ í™œìš©**: `core.constants`ì˜ OpenAI ê°€ê²© ì •ì±… ì‚¬ìš©
5. **ë²”ìš©ì„±**: summarizationë¿ë§Œ ì•„ë‹ˆë¼ ëª¨ë“  LLM ì‚¬ìš© ì‚¬ë¡€ì— ì ìš© ê°€ëŠ¥
