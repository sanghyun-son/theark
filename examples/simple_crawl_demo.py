"""Demo for simple crawl strategy: always start from yesterday and skip completed dates."""

import asyncio
import logging
from datetime import datetime
from pathlib import Path

from sqlalchemy.engine import Engine

from core.database.engine import create_database_engine, create_database_tables
from core.extractors.concrete.arxiv_source_explorer import ArxivSourceExplorer
from core.extractors.concrete.historical_crawl_manager import HistoricalCrawlManager
from core.types import Environment

# Set up logging
logging.basicConfig(level=logging.DEBUG, format="%(levelname)s:%(name)s:%(message)s")
logger = logging.getLogger(__name__)


async def main() -> None:
    """Run simple crawl demo."""
    print("ğŸš€ Simple Crawl Strategy Demo")
    print("=" * 50)
    print("ğŸ¯ Strategy:")
    print("   â€¢ Always start from yesterday")
    print("   â€¢ Skip already completed dates")
    print("   â€¢ Simple and intuitive")
    print("=" * 50)

    # Initialize database
    print("ğŸ”§ Initializing database...")
    engine: Engine = create_database_engine(
        Environment.DEVELOPMENT,
        db_path=Path("db/theark.demo.db"),
    )
    print("âœ… Database engine created")

    create_database_tables(engine)
    print("âœ… Database tables created")

    # Create crawl manager
    print("ğŸ”§ Creating crawl manager...")
    crawl_manager = HistoricalCrawlManager(
        categories=["cs.AI", "cs.LG"],
        rate_limit_delay=1.0,
        batch_size=100,  # 10 â†’ 100ìœ¼ë¡œ ì¦ê°€
    )
    print("âœ… Crawl manager created")

    print("\nğŸ“Š Initial state:")
    print(f"   End date: {crawl_manager.end_date}")
    print(f"   Today: {datetime.now().strftime('%Y-%m-%d')}")
    print("   Will start from: yesterday")

    # Create source explorer with REAL ArXiv API
    print("ğŸ”§ Creating ArXiv source explorer...")
    source_explorer = ArxivSourceExplorer(
        api_base_url="https://export.arxiv.org/api/query",  # Real ArXiv API
        delay_seconds=2.0,  # Be respectful to ArXiv API
        max_results_per_request=100,  # 10 â†’ 100ìœ¼ë¡œ ì¦ê°€
    )
    print("âœ… ArXiv source explorer created")

    print("\nğŸ”„ Running crawl cycles...")

    # Start the crawl manager
    print("ğŸš€ Starting crawl manager...")
    await crawl_manager.start(source_explorer, engine)
    print("âœ… Crawl manager started")

    # Wait a bit for initial crawl cycles
    print("â³ Waiting for initial crawl cycles...")
    await asyncio.sleep(50)

    # Check status
    print(
        f"ğŸ“Š Crawl manager status: {'Running' if crawl_manager.is_running else 'Stopped'}"
    )
    print(
        f"ğŸ“ˆ Current progress: {crawl_manager.current_date} | Category: {crawl_manager.current_category_index}"
    )

    # Stop the crawl manager
    print("ğŸ›‘ Stopping crawl manager...")
    await crawl_manager.stop()
    print("âœ… Crawl manager stopped")

    print("\nğŸ‰ Simple crawl demo completed!")


if __name__ == "__main__":
    asyncio.run(main())
